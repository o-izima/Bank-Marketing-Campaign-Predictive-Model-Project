{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cab12fd",
   "metadata": {},
   "source": [
    "# Bank-Marketing-Campaign-Predictive-Model-Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661640e",
   "metadata": {},
   "source": [
    "### Here we are importing the necessary librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920e7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from pandas import read_csv, get_dummies, Series\n",
    "from datetime import datetime\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_curve, roc_auc_score, confusion_matrix, plot_confusion_matrix, classification_report\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('png', 'pdf')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c124be",
   "metadata": {},
   "source": [
    "#### Read in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6009d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the CSV file in the sub-directory\n",
    "# file_path = \"data/bank-additional/bank-additional-full.csv\"\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "# #Check the top five rows in the dataset\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04de580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>subscription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital            education default housing loan  \\\n",
       "0   56  housemaid  married             basic.4y      no      no   no   \n",
       "1   37   services  married          high.school      no     yes   no   \n",
       "2   40     admin.  married             basic.6y      no      no   no   \n",
       "3   56   services  married          high.school      no      no  yes   \n",
       "4   59     admin.  married  professional.course      no      no   no   \n",
       "\n",
       "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "1  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "2  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "3  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "4  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \\\n",
       "0          1.1          93.994          -36.4      4.857       5191.0   \n",
       "1          1.1          93.994          -36.4      4.857       5191.0   \n",
       "2          1.1          93.994          -36.4      4.857       5191.0   \n",
       "3          1.1          93.994          -36.4      4.857       5191.0   \n",
       "4          1.1          93.994          -36.4      4.857       5191.0   \n",
       "\n",
       "   subscription  \n",
       "0            no  \n",
       "1            no  \n",
       "2            no  \n",
       "3            no  \n",
       "4            no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"data/df_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98fe31",
   "metadata": {},
   "source": [
    "### Checking the dataset shape in rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc7d3b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30478, 21)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f1f9b",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28d1b95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 int64\n",
       "job                object\n",
       "marital            object\n",
       "education          object\n",
       "default            object\n",
       "housing            object\n",
       "loan               object\n",
       "contact            object\n",
       "month              object\n",
       "day_of_week        object\n",
       "duration            int64\n",
       "campaign            int64\n",
       "pdays               int64\n",
       "previous            int64\n",
       "poutcome           object\n",
       "emp.var.rate      float64\n",
       "cons.price.idx    float64\n",
       "cons.conf.idx     float64\n",
       "euribor3m         float64\n",
       "nr.employed       float64\n",
       "subscription       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c118df",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ac43",
   "metadata": {},
   "source": [
    "### Missing values, unknown entries, nulls and any NaN have been removed in the EDA file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318f01a",
   "metadata": {},
   "source": [
    "## 2. Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babb407",
   "metadata": {},
   "source": [
    "#### Create a list of categorical values that need encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "659d85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating a list of the categorical features '''\n",
    "obj_cols = []\n",
    "for i in df:\n",
    "     if df[i].dtypes == 'object':\n",
    "        obj_cols.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f994014",
   "metadata": {},
   "source": [
    "#### Let's find out the number of classes in each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14421e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job : 11\n",
      "marital : 3\n",
      "education : 7\n",
      "default : 2\n",
      "housing : 2\n",
      "loan : 2\n",
      "contact : 2\n",
      "month : 10\n",
      "day_of_week : 5\n",
      "poutcome : 3\n",
      "subscription : 2\n"
     ]
    }
   ],
   "source": [
    "for k in obj_cols:\n",
    "    print(k, \":\", df[k].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a30e6",
   "metadata": {},
   "source": [
    "#### Let's print out the number of classes in each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1da5fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job : ['housemaid' 'services' 'admin.' 'technician' 'blue-collar' 'unemployed'\n",
      " 'retired' 'entrepreneur' 'management' 'student' 'self-employed'] \n",
      "\n",
      "marital : ['married' 'single' 'divorced'] \n",
      "\n",
      "education : ['basic.4y' 'high.school' 'basic.6y' 'professional.course' 'basic.9y'\n",
      " 'university.degree' 'illiterate'] \n",
      "\n",
      "default : ['no' 'yes'] \n",
      "\n",
      "housing : ['no' 'yes'] \n",
      "\n",
      "loan : ['no' 'yes'] \n",
      "\n",
      "contact : ['telephone' 'cellular'] \n",
      "\n",
      "month : ['may' 'jun' 'jul' 'aug' 'oct' 'nov' 'dec' 'mar' 'apr' 'sep'] \n",
      "\n",
      "day_of_week : ['mon' 'tue' 'wed' 'thu' 'fri'] \n",
      "\n",
      "poutcome : ['nonexistent' 'failure' 'success'] \n",
      "\n",
      "subscription : ['no' 'yes'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in obj_cols:\n",
    "    print(k, \":\", df[k].unique(), '\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0c648",
   "metadata": {},
   "source": [
    "##### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7691705",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df.marital = le.fit_transform(df.marital)\n",
    "df.housing = le.fit_transform(df.housing)\n",
    "df.subscription = le.fit_transform(df.subscription)\n",
    "df.loan = le.fit_transform(df.loan)\n",
    "df.default = le.fit_transform(df.default)\n",
    "df.contact = le.fit_transform(df.contact)\n",
    "df.poutcome = le.fit_transform(df.poutcome)\n",
    "df.day_of_week = le.fit_transform(df.day_of_week)\n",
    "df.month = le.fit_transform(df.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f107349",
   "metadata": {},
   "source": [
    "##### Encode Job and education with One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ca25186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>marital</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>...</th>\n",
       "      <th>job_student</th>\n",
       "      <th>job_technician</th>\n",
       "      <th>job_unemployed</th>\n",
       "      <th>education_basic.4y</th>\n",
       "      <th>education_basic.6y</th>\n",
       "      <th>education_basic.9y</th>\n",
       "      <th>education_high.school</th>\n",
       "      <th>education_illiterate</th>\n",
       "      <th>education_professional.course</th>\n",
       "      <th>education_university.degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  marital  default  housing  loan  contact  month  day_of_week  \\\n",
       "0   56        1        0        0     0        1      6            1   \n",
       "1   37        1        0        1     0        1      6            1   \n",
       "2   40        1        0        0     0        1      6            1   \n",
       "3   56        1        0        0     1        1      6            1   \n",
       "4   59        1        0        0     0        1      6            1   \n",
       "\n",
       "   duration  campaign  ...  job_student  job_technician  job_unemployed  \\\n",
       "0       261         1  ...            0               0               0   \n",
       "1       226         1  ...            0               0               0   \n",
       "2       151         1  ...            0               0               0   \n",
       "3       307         1  ...            0               0               0   \n",
       "4       139         1  ...            0               0               0   \n",
       "\n",
       "   education_basic.4y  education_basic.6y  education_basic.9y  \\\n",
       "0                   1                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   1                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   education_high.school  education_illiterate  education_professional.course  \\\n",
       "0                      0                     0                              0   \n",
       "1                      1                     0                              0   \n",
       "2                      0                     0                              0   \n",
       "3                      1                     0                              0   \n",
       "4                      0                     0                              1   \n",
       "\n",
       "   education_university.degree  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['job','education']\n",
    "\n",
    "dataset = pd.get_dummies(df, columns=categorical_features)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e9a7ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30478, 37)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259ae81",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a87f0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('subscription', axis=1) # Obtain Feature Set\n",
    "y = dataset['subscription'] # Extract Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4fb24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30478, 36)\n",
      "(30478,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09461bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'marital', 'default', 'housing', 'loan', 'contact', 'month',\n",
       "       'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome',\n",
       "       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n",
       "       'nr.employed', 'subscription', 'job_admin.', 'job_blue-collar',\n",
       "       'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired',\n",
       "       'job_self-employed', 'job_services', 'job_student', 'job_technician',\n",
       "       'job_unemployed', 'education_basic.4y', 'education_basic.6y',\n",
       "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
       "       'education_professional.course', 'education_university.degree'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain a list of dataset columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ba31ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cols = ['age', 'marital', 'default', 'housing', 'loan', 'contact', 'month',\n",
    "       'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome',\n",
    "       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n",
    "       'nr.employed', 'job_admin.', 'job_blue-collar',\n",
    "       'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired',\n",
    "       'job_self-employed', 'job_services', 'job_student', 'job_technician',\n",
    "       'job_unemployed', 'education_basic.4y', 'education_basic.6y',\n",
    "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
    "       'education_professional.course', 'education_university.degree']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7b562",
   "metadata": {},
   "source": [
    "## 4. Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e04fc0",
   "metadata": {},
   "source": [
    "##### The features are standardized using the Scikit-Learn StandardScaler method in Python. \n",
    "Data scaling or standardization is essential to ensure that each contributing feature has a fair and balanced impact, preventing any feature from dominating the others. This process effectively eliminates potential biases. The StandardScaler method scales the feature set by subtracting the mean and normalizing it to have unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73c14621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>marital</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>...</th>\n",
       "      <th>job_student</th>\n",
       "      <th>job_technician</th>\n",
       "      <th>job_unemployed</th>\n",
       "      <th>education_basic.4y</th>\n",
       "      <th>education_basic.6y</th>\n",
       "      <th>education_basic.9y</th>\n",
       "      <th>education_high.school</th>\n",
       "      <th>education_illiterate</th>\n",
       "      <th>education_professional.course</th>\n",
       "      <th>education_university.degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.642408</td>\n",
       "      <td>-0.309721</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-1.087624</td>\n",
       "      <td>-0.430643</td>\n",
       "      <td>1.426447</td>\n",
       "      <td>0.734364</td>\n",
       "      <td>-0.725499</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>-0.559363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14291</td>\n",
       "      <td>-0.467634</td>\n",
       "      <td>-0.157528</td>\n",
       "      <td>3.435969</td>\n",
       "      <td>-0.218435</td>\n",
       "      <td>-0.403972</td>\n",
       "      <td>-0.581265</td>\n",
       "      <td>-0.019001</td>\n",
       "      <td>-0.406277</td>\n",
       "      <td>-0.720128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.196436</td>\n",
       "      <td>-0.309721</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>0.919436</td>\n",
       "      <td>-0.430643</td>\n",
       "      <td>1.426447</td>\n",
       "      <td>0.734364</td>\n",
       "      <td>-0.725499</td>\n",
       "      <td>-0.128039</td>\n",
       "      <td>-0.559363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14291</td>\n",
       "      <td>-0.467634</td>\n",
       "      <td>-0.157528</td>\n",
       "      <td>-0.291039</td>\n",
       "      <td>-0.218435</td>\n",
       "      <td>-0.403972</td>\n",
       "      <td>1.720385</td>\n",
       "      <td>-0.019001</td>\n",
       "      <td>-0.406277</td>\n",
       "      <td>-0.720128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093908</td>\n",
       "      <td>-0.309721</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-1.087624</td>\n",
       "      <td>-0.430643</td>\n",
       "      <td>1.426447</td>\n",
       "      <td>0.734364</td>\n",
       "      <td>-0.725499</td>\n",
       "      <td>-0.414580</td>\n",
       "      <td>-0.559363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14291</td>\n",
       "      <td>-0.467634</td>\n",
       "      <td>-0.157528</td>\n",
       "      <td>-0.291039</td>\n",
       "      <td>4.578014</td>\n",
       "      <td>-0.403972</td>\n",
       "      <td>-0.581265</td>\n",
       "      <td>-0.019001</td>\n",
       "      <td>-0.406277</td>\n",
       "      <td>-0.720128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.642408</td>\n",
       "      <td>-0.309721</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-1.087624</td>\n",
       "      <td>2.322111</td>\n",
       "      <td>1.426447</td>\n",
       "      <td>0.734364</td>\n",
       "      <td>-0.725499</td>\n",
       "      <td>0.181426</td>\n",
       "      <td>-0.559363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14291</td>\n",
       "      <td>-0.467634</td>\n",
       "      <td>-0.157528</td>\n",
       "      <td>-0.291039</td>\n",
       "      <td>-0.218435</td>\n",
       "      <td>-0.403972</td>\n",
       "      <td>1.720385</td>\n",
       "      <td>-0.019001</td>\n",
       "      <td>-0.406277</td>\n",
       "      <td>-0.720128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.932752</td>\n",
       "      <td>-0.309721</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-1.087624</td>\n",
       "      <td>-0.430643</td>\n",
       "      <td>1.426447</td>\n",
       "      <td>0.734364</td>\n",
       "      <td>-0.725499</td>\n",
       "      <td>-0.460427</td>\n",
       "      <td>-0.559363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14291</td>\n",
       "      <td>-0.467634</td>\n",
       "      <td>-0.157528</td>\n",
       "      <td>-0.291039</td>\n",
       "      <td>-0.218435</td>\n",
       "      <td>-0.403972</td>\n",
       "      <td>-0.581265</td>\n",
       "      <td>-0.019001</td>\n",
       "      <td>2.461374</td>\n",
       "      <td>-0.720128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age   marital   default   housing      loan   contact     month  \\\n",
       "0  1.642408 -0.309721 -0.009922 -1.087624 -0.430643  1.426447  0.734364   \n",
       "1 -0.196436 -0.309721 -0.009922  0.919436 -0.430643  1.426447  0.734364   \n",
       "2  0.093908 -0.309721 -0.009922 -1.087624 -0.430643  1.426447  0.734364   \n",
       "3  1.642408 -0.309721 -0.009922 -1.087624  2.322111  1.426447  0.734364   \n",
       "4  1.932752 -0.309721 -0.009922 -1.087624 -0.430643  1.426447  0.734364   \n",
       "\n",
       "   day_of_week  duration  campaign  ...  job_student  job_technician  \\\n",
       "0    -0.725499  0.005681 -0.559363  ...     -0.14291       -0.467634   \n",
       "1    -0.725499 -0.128039 -0.559363  ...     -0.14291       -0.467634   \n",
       "2    -0.725499 -0.414580 -0.559363  ...     -0.14291       -0.467634   \n",
       "3    -0.725499  0.181426 -0.559363  ...     -0.14291       -0.467634   \n",
       "4    -0.725499 -0.460427 -0.559363  ...     -0.14291       -0.467634   \n",
       "\n",
       "   job_unemployed  education_basic.4y  education_basic.6y  education_basic.9y  \\\n",
       "0       -0.157528            3.435969           -0.218435           -0.403972   \n",
       "1       -0.157528           -0.291039           -0.218435           -0.403972   \n",
       "2       -0.157528           -0.291039            4.578014           -0.403972   \n",
       "3       -0.157528           -0.291039           -0.218435           -0.403972   \n",
       "4       -0.157528           -0.291039           -0.218435           -0.403972   \n",
       "\n",
       "   education_high.school  education_illiterate  education_professional.course  \\\n",
       "0              -0.581265             -0.019001                      -0.406277   \n",
       "1               1.720385             -0.019001                      -0.406277   \n",
       "2              -0.581265             -0.019001                      -0.406277   \n",
       "3               1.720385             -0.019001                      -0.406277   \n",
       "4              -0.581265             -0.019001                       2.461374   \n",
       "\n",
       "   education_university.degree  \n",
       "0                    -0.720128  \n",
       "1                    -0.720128  \n",
       "2                    -0.720128  \n",
       "3                    -0.720128  \n",
       "4                    -0.720128  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X.astype(float))\n",
    "X_scaled = pd.DataFrame(X_scaled,columns=dataset_cols)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec969485",
   "metadata": {},
   "source": [
    "### Here, the dataset is split to training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3045e",
   "metadata": {},
   "source": [
    "#### The dataset is split into two set: the training data and the testing data using a ratio of $70$\\% to $30$\\%. \n",
    "\n",
    "The $70$\\% in the training data will be used for model training. The remaining $30$\\% has been set aside for testing the data after training. \n",
    "\n",
    "This is done to evaluate the model's performance when presented with unseen data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bff0de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21334, 36)\n",
      "(9144, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2b40fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21334,)\n",
      "(9144,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351980b",
   "metadata": {},
   "source": [
    "## 5. Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b6cd3",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE) is used to balance the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90e1d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying SMOTE, number in each class of the target: \n",
      " 0    18661\n",
      "1     2673\n",
      "Name: subscription, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Before applying SMOTE, let's check the class distribution in the training set\n",
    "print(\"Before applying SMOTE, number in each class of the target: \\n\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9bbdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d062940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62586e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying SMOTE, number in each class of the target: \n",
      " 0    18661\n",
      "1    18661\n",
      "Name: subscription, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"After applying SMOTE, number in each class of the target: \\n\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37918cc1",
   "metadata": {},
   "source": [
    "## Create a directory to save cleaned, encoded and balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0335847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"cleaned_data_final\"):\n",
    "#     os.makedirs(\"cleaned_data_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "# X_train.to_csv(\"cleaned_data_final/train_features.csv\", index=False)\n",
    "# X_test.to_csv(\"cleaned_data_final/test_features.csv\", index=False)\n",
    "\n",
    "# y_train.to_csv(\"cleaned_data_final/train_labels.csv\", index=False)\n",
    "# y_test.to_csv(\"cleaned_data_final/test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a4752",
   "metadata": {},
   "source": [
    "# Read in saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00118b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"cleaned_data/train_features.csv\")\n",
    "X_test = pd.read_csv(\"cleaned_data/test_features.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"cleaned_data/train_labels.csv\")\n",
    "y_test = pd.read_csv(\"cleaned_data/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff83ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37322, 36)\n",
      "(9144, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef59c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37322, 1)\n",
      "(9144, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de85280",
   "metadata": {},
   "source": [
    "## Change the labels back to the default \"Series\" type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b91ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0242a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37322,)\n",
      "(9144,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e7e77",
   "metadata": {},
   "source": [
    "# Machine Learning (ML) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d8f00",
   "metadata": {},
   "source": [
    "### Three classes of ML models were used in this project. In general, eight (8) ML models were employed for the study.\n",
    "\n",
    "- **Single-Base Classifiers**\n",
    "    - Decision Tree (DT)\n",
    "    - Support Vector Classifier (SVC)\n",
    "    - Naïve Bayes (NB)\n",
    "    - Logistic Regression (LR)\n",
    "\n",
    "\n",
    "- __Traditional Ensemble Model__\n",
    "    - Random Forest Classifier (RFC)\n",
    "    \n",
    "    \n",
    "- **Boosting Models**\n",
    "    - Gradient Boosting Machines (GBM)\n",
    "    - Light Gradient Boosting Machines (LightGBM)\n",
    "    - eXtreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b357287",
   "metadata": {},
   "source": [
    "'''   \n",
    "Ensemble models are a machine learning technique that combines the predictions from multiple individual models to improve overall predictive performance. The idea behind ensemble methods is to leverage the wisdom of the crowd – by combining the opinions of multiple models, the ensemble can often make more accurate predictions than any individual model.\n",
    "\n",
    "There are several popular ensemble methods, including:\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)**: Bagging involves training multiple instances of the same base model on different subsets of the training data. These subsets are created through resampling with replacement (bootstrap samples). The predictions from each model are then averaged (for regression) or voted upon (for classification) to make the final prediction. Random Forest is a well-known algorithm that uses bagging.\n",
    "\n",
    "**Boosting**: Boosting aims to correct the errors of previous models by giving more weight to misclassified data points. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00a138",
   "metadata": {},
   "source": [
    "# Model Evaluation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171d275",
   "metadata": {},
   "source": [
    "### Six Model Evaluation Strategies were adopted for this work\n",
    "1. Models were evaluated using the default settings\n",
    "2. Models were evaluated using the bagging technique\n",
    "3. Models were evaluated using the reduced dataset\n",
    "4. Models were evaluated by applying hyperparameter tuning\n",
    "5. Models were evaluated using the Cross-Validation via the GridSearchCV (5-fold)\n",
    "6. Models were evaluated using the top ten features selected by RFC and GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53129ddc",
   "metadata": {},
   "source": [
    "## 1. Evaluating the models using the default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cab9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproduability\n",
    "default = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8958d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Random Forest...\n",
      "Training and testing LightGBM...\n",
      "Training and testing SVC...\n",
      "Training and testing Naive Bayes...\n",
      "Training and testing Decision Tree...\n",
      "Training and testing XGBoost...\n",
      "Training and testing Logistic Regression...\n",
      "Training and testing Gradient Boosting...\n",
      "                     Accuracy Precision    Recall  F1-Score\n",
      "Random Forest        0.902668  0.620212  0.642194  0.631012\n",
      "LightGBM             0.908683  0.646321  0.652321  0.649307\n",
      "SVC                   0.85783  0.471689  0.808439  0.595771\n",
      "Naive Bayes          0.393045  0.170665   0.95443  0.289555\n",
      "Decision Tree        0.873469  0.510355  0.582278   0.54395\n",
      "XGBoost              0.901794  0.634489  0.571308  0.601243\n",
      "Logistic Regression  0.857393  0.471545  0.832068  0.601954\n",
      "Gradient Boosting    0.895669  0.575244  0.745148  0.649265\n"
     ]
    }
   ],
   "source": [
    "# # Create a directory to store the figures\n",
    "if not os.path.exists(\"default_settings\"):\n",
    "    os.makedirs(\"default_settings\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=default)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=default)),\n",
    "    ('SVC', SVC(random_state=default)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=default)),\n",
    "    ('XGBoost', XGBClassifier(random_state=default)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=default)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=default))\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and test each model, and compute metrics\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "\n",
    "       \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'default_settings/confusion_matrix_{model_name}.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "\n",
    "results_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c072c27",
   "metadata": {},
   "source": [
    "# 2. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07efcd",
   "metadata": {},
   "source": [
    "#### Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple instances of the same model on different bootstrap samples of the training data and then combining their predictions to make a final decision. \n",
    "\n",
    "First, we create a BaggingClassifier for each base model defined in the models list. We then fit the BaggingClassifier on the training data and make predictions on the test data. We compute various classification metrics for the bagging ensemble and store the results in the results dictionary. The confusion matrix plots for each bagging ensemble are also saved in the \"bagged_mod\" folder.\n",
    "\n",
    "The BaggingClassifier automatically performs bootstrap aggregation, creating multiple instances of the base model, each trained on a different bootstrap sample of the training data. It aggregates the predictions of the individual models to make a final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3863d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproduability\n",
    "bagged = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "556cc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Random Forest with Bagging...\n",
      "Training and testing LightGBM with Bagging...\n",
      "Training and testing SVC with Bagging...\n",
      "Training and testing Naive Bayes with Bagging...\n",
      "Training and testing Decision Tree with Bagging...\n",
      "Training and testing XGBoost with Bagging...\n",
      "Training and testing Logistic Regression with Bagging...\n",
      "Training and testing Gradient Boosting with Bagging...\n",
      "                     Accuracy Precision    Recall  F1-Score\n",
      "Random Forest        0.901465  0.605341  0.688608  0.644295\n",
      "LightGBM             0.909121  0.644608  0.665823  0.655044\n",
      "SVC                   0.85958  0.475063  0.795781  0.594953\n",
      "Naive Bayes          0.401903  0.172377  0.951055  0.291855\n",
      "Decision Tree        0.894685  0.590244  0.612658  0.601242\n",
      "XGBoost              0.907043  0.652689  0.604219   0.62752\n",
      "Logistic Regression  0.858049  0.472928  0.832911  0.603301\n",
      "Gradient Boosting    0.895997   0.57529   0.75443  0.652793\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the figures\n",
    "if not os.path.exists(\"bagged_model\"):\n",
    "    os.makedirs(\"bagged_model\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=bagged)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=bagged)),\n",
    "    ('SVC', SVC(random_state=bagged)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=bagged)),\n",
    "    ('XGBoost', XGBClassifier(random_state=bagged)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=bagged)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=bagged))\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and test each model with Bagging, and compute metrics\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name} with Bagging...\")\n",
    "    \n",
    "    # Applying Bagging to the model\n",
    "    bagging_model = BaggingClassifier(base_estimator=model, n_estimators=10, random_state=42)\n",
    "    \n",
    "    bagging_model.fit(X_train, y_train)\n",
    "    y_pred = bagging_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name} with Bagging')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'bagged_model/confusion_matrix_{model_name}_with_bagging.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "bag_results_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(bag_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc46790",
   "metadata": {},
   "source": [
    "## 3. Reduced dataset by half (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "227926ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming X and y are your feature and target data in a DataFrame\n",
    "# subset_size = int(len(X_train) * 0.5)  # Choose the desired subset size (e.g., 50% of the data)\n",
    "\n",
    "# # Sample a random subset of data\n",
    "# subset_indices = X_train.sample(n=subset_size, random_state=100).index\n",
    "\n",
    "# # Select the subset of data\n",
    "# X_subset = X_train.loc[subset_indices]\n",
    "# y_subset = y_train.loc[subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random selection of a susbet of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29e8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your feature and target data in a DataFrame\n",
    "subset_size = int(len(X_train) * 0.5)  # Choose the desired subset size (e.g., 50% of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bea3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random subset of data\n",
    "subset_indices = X_train.sample(n=subset_size, random_state=100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0e7857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the subset of data -- Training data\n",
    "X_subset = X_train.loc[subset_indices]\n",
    "y_subset = y_train.loc[subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d9d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18661, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aac8be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18661,)\n"
     ]
    }
   ],
   "source": [
    "print(y_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635e9e6",
   "metadata": {},
   "source": [
    "## Subset of Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12670730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9144, 36)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92c152bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = int(len(X_test) * 0.5)\n",
    "# Sample a random subset of data\n",
    "sub_ind = X_test.sample(n=test_sub, random_state=200).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01e6cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the subset of data -- Training data\n",
    "X_subset_test = X_test.loc[sub_ind]\n",
    "y_subset_test = y_test.loc[sub_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6ebc34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4572, 36)\n",
      "(4572,)\n"
     ]
    }
   ],
   "source": [
    "print(X_subset_test.shape)\n",
    "print(y_subset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467e5bf7",
   "metadata": {},
   "source": [
    "## 3.1 Default with reduced dataset (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e277427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_half = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1403f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Random Forest...\n",
      "Training and testing LightGBM...\n",
      "Training and testing SVC...\n",
      "Training and testing Naive Bayes...\n",
      "Training and testing Decision Tree...\n",
      "Training and testing XGBoost...\n",
      "Training and testing Logistic Regression...\n",
      "Training and testing Gradient Boosting...\n",
      "                     Accuracy Precision    Recall  F1-Score\n",
      "Random Forest        0.901137  0.599013    0.7173  0.652842\n",
      "LightGBM             0.905074  0.629599  0.649789  0.639535\n",
      "SVC                  0.854003  0.464455  0.827004  0.594841\n",
      "Naive Bayes          0.814414  0.376089  0.655696  0.478007\n",
      "Decision Tree        0.867345  0.490331  0.599156  0.539309\n",
      "XGBoost              0.900262  0.620053  0.594937  0.607235\n",
      "Logistic Regression  0.857065  0.470897  0.832911  0.601646\n",
      "Gradient Boosting    0.895232  0.573845  0.744304  0.648053\n"
     ]
    }
   ],
   "source": [
    "# # Create a directory to store the figures\n",
    "if not os.path.exists(\"default_half\"):\n",
    "    os.makedirs(\"default_half\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=def_half)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=def_half)),\n",
    "    ('SVC', SVC(random_state=def_half)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=def_half)),\n",
    "    ('XGBoost', XGBClassifier(random_state=def_half)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=def_half)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=def_half))\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and test each model, and compute metrics\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    model.fit(X_subset, y_subset)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "\n",
    "       \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'default_half/confusion_matrix_{model_name}.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "\n",
    "results_def_half = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_def_half)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02822b63",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497ad06",
   "metadata": {},
   "source": [
    "## 4.1 Hyperparameter Tuning -- LightGBM, XGB and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "133d3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc319016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing LightGBM...\n",
      "The best parameters for the LightGBM... are  {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100} \n",
      "\n",
      "Training and testing XGBoost...\n",
      "The best parameters for the XGBoost... are  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300} \n",
      "\n",
      "Training and testing SVC...\n",
      "The best parameters for the SVC... are  {'C': 10, 'kernel': 'rbf'} \n",
      "\n",
      "          Accuracy Precision    Recall  F1-Score\n",
      "LightGBM   0.90584  0.629808  0.663291  0.646116\n",
      "XGBoost   0.904528  0.635182  0.618565  0.626764\n",
      "SVC       0.867563  0.492672  0.737553   0.59074\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the figures\n",
    "if not os.path.exists(\"tuned_mod\"):\n",
    "    os.makedirs(\"tuned_mod\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('LightGBM', LGBMClassifier(random_state=tuned)),\n",
    "    ('XGBoost', XGBClassifier(random_state=tuned)),\n",
    "    ('SVC', SVC(random_state=tuned))   \n",
    "]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "lgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 0.7, 0.99],\n",
    "    'max_depth': [3, 5, 10, 12, 15]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for SVC\n",
    "svc_params = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    if  model_name == 'LightGBM':\n",
    "        grid_search = GridSearchCV(model, lgb_params, cv=5, n_jobs=-1)\n",
    "    elif model_name == 'XGBoost':\n",
    "        grid_search = GridSearchCV(model, xgb_params, cv=5, n_jobs=-1)\n",
    "    elif model_name == 'SVC':\n",
    "        grid_search = GridSearchCV(model, svc_params, cv=5, n_jobs=-1)\n",
    "    else:\n",
    "        grid_search = model\n",
    "    \n",
    "    grid_search.fit(X_subset, y_subset)\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "    \n",
    "    print(f\"The best parameters for the {model_name}...\", \"are \", grid_search.best_params_, \"\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'tuned_mod/confusion_matrix_{model_name}_with_tuning.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "results_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd8720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44abcd21",
   "metadata": {},
   "source": [
    "## 4.2 Hyperparameter Tuning -- RFC and DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b3d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2028218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Random Forest...\n",
      "The best parameters for the Random Forest... are  {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} \n",
      "\n",
      "Training and testing Decision Tree...\n",
      "The best parameters for the Decision Tree... are  {'criterion': 'gini', 'max_depth': 10} \n",
      "\n",
      "               Accuracy Precision    Recall  F1-Score\n",
      "Random Forest  0.897638  0.588865  0.696203  0.638051\n",
      "Decision Tree  0.876094  0.515099  0.748523  0.610251\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the figures\n",
    "if not os.path.exists(\"tuned_mod\"):\n",
    "    os.makedirs(\"tuned_mod\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=tuned)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=tuned))\n",
    "   ]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Hyperparameter tuning for RandomForestClassifier\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for DecisionTreeClassifier\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        grid_search = GridSearchCV(model, rf_params, cv=5, n_jobs=-1)\n",
    "    elif model_name == 'Decision Tree':\n",
    "        grid_search = GridSearchCV(model, dt_params, cv=5, n_jobs=-1)\n",
    "    else:\n",
    "        grid_search = model\n",
    "    \n",
    "    grid_search.fit(X_subset, y_subset)\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "    \n",
    "    print(f\"The best parameters for the {model_name}...\", \"are \", grid_search.best_params_, \"\\n\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'tuned_mod/confusion_matrix_{model_name}_with_tuning.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "results_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f416ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c8f34f",
   "metadata": {},
   "source": [
    "## 4.3 Hyperparameter Tuning -- Logistic Regression and GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c066813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Logistic Regression...\n",
      "The best parameters for the Logistic Regression... are  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'} \n",
      "\n",
      "Training and testing Gradient Boosting...\n",
      "The best parameters for the Gradient Boosting... are  {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200} \n",
      "\n",
      "                     Accuracy Precision    Recall  F1-Score\n",
      "Logistic Regression  0.857065  0.470925  0.833755  0.601889\n",
      "Gradient Boosting    0.898731  0.597736  0.668354  0.631076\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the figures\n",
    "if not os.path.exists(\"tuned_mod\"):\n",
    "    os.makedirs(\"tuned_mod\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=tuned)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=tuned))\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Hyperparameter tuning for GradientBoostingClassifier\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "log_reg_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    if model_name == 'Gradient Boosting':\n",
    "        grid_search = GridSearchCV(model, gb_params, cv=5, n_jobs=-1)\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        grid_search = GridSearchCV(model, log_reg_params, cv=5, n_jobs=-1)\n",
    "    else:\n",
    "        grid_search = model\n",
    "    \n",
    "    grid_search.fit(X_subset, y_subset)\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "    \n",
    "    print(f\"The best parameters for the {model_name}...\", \"are \", grid_search.best_params_, \"\\n\")\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'tuned_mod/confusion_matrix_{model_name}_with_tuning.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "results_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bd2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16584526",
   "metadata": {},
   "source": [
    "# Cross-Validation using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cdc71",
   "metadata": {},
   "source": [
    "## 5.0 Finding the best number of trees using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7f60d",
   "metadata": {},
   "source": [
    "### XGBoost, LightGBM, GBM and RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "449aded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'classifier__n_estimators': 200}\n",
      "Best Score for XGBoost: 0.9449090289561031\n",
      "Best Parameters for GBM: {'classifier__n_estimators': 200}\n",
      "Best Score for GBM: 0.9487756082176917\n",
      "Best Parameters for LightGBM: {'classifier__n_estimators': 200}\n",
      "Best Score for LightGBM: 0.9448021941211436\n",
      "Best Parameters for RandomForest: {'classifier__n_estimators': 450}\n",
      "Best Score for RandomForest: 0.9618770298186229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a dictionary of classifiers\n",
    "classifiers = {\n",
    "    'XGBoost': XGBClassifier(random_state=103),\n",
    "    'GBM': GradientBoostingClassifier(random_state=103),\n",
    "    'LightGBM': LGBMClassifier(random_state=103),\n",
    "    'RandomForest': RandomForestClassifier(criterion='entropy', max_features='auto', random_state=103)\n",
    "}\n",
    "\n",
    "# Create a dictionary of hyperparameter grids for each classifier\n",
    "param_grids = {\n",
    "    'XGBoost': {'classifier__n_estimators': [200, 250, 300, 350, 400, 450, 500]},  # Set n_estimators for XGBoost\n",
    "    'GBM': {'classifier__n_estimators': [200, 250, 300, 350, 400, 450, 500]},\n",
    "    'LightGBM': {'classifier__n_estimators': [200, 250, 300, 350, 400, 450, 500]},\n",
    "    'RandomForest': {'classifier__n_estimators': [200, 250, 300, 350, 400, 450, 500]}\n",
    "}\n",
    "\n",
    "# Create a pipeline with each classifier and perform grid search\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    param_grid = param_grids[clf_name]\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='recall', cv=5)\n",
    "    grid_search.fit(X_subset, y_subset)\n",
    "    \n",
    "    best_parameters = grid_search.best_params_\n",
    "    best_result = grid_search.best_score_\n",
    "    \n",
    "    best_params[clf_name] = best_parameters\n",
    "    best_scores[clf_name] = best_result\n",
    "\n",
    "# Print the best parameters and scores for each classifier\n",
    "for clf_name in classifiers.keys():\n",
    "    print(f\"Best Parameters for {clf_name}: {best_params[clf_name]}\")\n",
    "    print(f\"Best Score for {clf_name}: {best_scores[clf_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e95a3",
   "metadata": {},
   "source": [
    "## 5.1 Fitting models with the best number of trees obtained via GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "678aa1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing Random Forest...\n",
      "Training and testing LightGBM...\n",
      "Training and testing XGBoost...\n",
      "Training and testing Gradient Boosting...\n",
      "                   Accuracy Precision    Recall  F1-Score\n",
      "Random Forest      0.893045   0.57784  0.686667   0.62757\n",
      "LightGBM             0.9007  0.624573      0.61  0.617201\n",
      "XGBoost            0.894357   0.60354  0.568333  0.585408\n",
      "Gradient Boosting  0.899825  0.606607  0.673333  0.638231\n"
     ]
    }
   ],
   "source": [
    "# # Create a directory to store the figures\n",
    "if not os.path.exists(\"grid_search\"):\n",
    "    os.makedirs(\"grid_search\")\n",
    "\n",
    "# Define the models to be used in the pipeline\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=450, random_state=103)),\n",
    "    ('LightGBM', LGBMClassifier(n_estimators=200, random_state=103)),\n",
    "    ('XGBoost', XGBClassifier(n_estimators=200, random_state=103)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=200, random_state=103))\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and test each model, and compute metrics\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and testing {model_name}...\")\n",
    "    \n",
    "    model.fit(X_subset, y_subset)\n",
    "    y_pred = model.predict(X_subset_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_subset_test, y_pred)\n",
    "    precision = precision_score(y_subset_test, y_pred)\n",
    "    recall = recall_score(y_subset_test, y_pred)\n",
    "    f1 = f1_score(y_subset_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_subset_test, y_pred)\n",
    "    classification_rep = classification_report(y_subset_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "\n",
    "       \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.set(style='white')\n",
    "    sns.heatmap(np.eye(2), annot=conf_matrix, fmt='g', annot_kws={'size': 20},\n",
    "            cmap=sns.color_palette(\"light:b\", as_cmap=True), cbar=False, ax=ax)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    additional_texts = ['(TN)', '(FP)', '(FN)', '(TP)']\n",
    "    for text_elt, additional_text in zip(ax.texts, additional_texts):\n",
    "        ax.text(*text_elt.get_position(), '\\n' + additional_text, color=text_elt.get_color(),\n",
    "            ha='center', va='top', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'grid_search/confusion_matrix_{model_name}_with_GridSearch_CV.pdf')\n",
    "    plt.close()\n",
    "\n",
    "# Convert results to a DataFrame and print\n",
    "\n",
    "results_grid = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "print(results_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863234b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ece8e2",
   "metadata": {},
   "source": [
    "## 6 Feature Selection for the top boosting model, GBM and RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de8cba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd743ec",
   "metadata": {},
   "source": [
    "## 6.1 RFC Model with Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1811a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration                         0.322755\n",
      "euribor3m                        0.094318\n",
      "nr.employed                      0.075100\n",
      "emp.var.rate                     0.057475\n",
      "campaign                         0.051207\n",
      "cons.conf.idx                    0.046478\n",
      "age                              0.042119\n",
      "day_of_week                      0.040727\n",
      "cons.price.idx                   0.038487\n",
      "month                            0.038290\n",
      "contact                          0.025187\n",
      "marital                          0.023795\n",
      "housing                          0.020140\n",
      "pdays                            0.017629\n",
      "poutcome                         0.016371\n",
      "previous                         0.010853\n",
      "loan                             0.009429\n",
      "education_university.degree      0.008291\n",
      "job_admin.                       0.007291\n",
      "education_high.school            0.006987\n",
      "job_technician                   0.005926\n",
      "education_professional.course    0.005411\n",
      "job_blue-collar                  0.005394\n",
      "education_basic.9y               0.005069\n",
      "job_management                   0.003709\n",
      "job_services                     0.003510\n",
      "education_basic.4y               0.003471\n",
      "job_retired                      0.003104\n",
      "job_self-employed                0.002020\n",
      "education_basic.6y               0.001950\n",
      "job_unemployed                   0.001945\n",
      "job_entrepreneur                 0.001868\n",
      "job_housemaid                    0.001836\n",
      "job_student                      0.001738\n",
      "education_illiterate             0.000119\n",
      "default                          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Let's obtain the most important features\n",
    "imp_features = Series(rfc.feature_importances_, index=list(X_subset)).sort_values(ascending=False)\n",
    "print(imp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d65cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------RFC Model with top ten features----------\n",
      "\n",
      "\n",
      "      Metric     Value\n",
      "0   Accuracy  0.888889\n",
      "1     Recall  0.640000\n",
      "2  Precision  0.568047\n",
      "3   F1-Score  0.601881\n",
      "\n",
      "\n",
      "['duration', 'euribor3m', 'nr.employed', 'emp.var.rate', 'cons.conf.idx', 'campaign', 'age', 'day_of_week', 'cons.price.idx', 'month']\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=450, random_state=42)\n",
    "rf_classifier.fit(X_subset, y_subset)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Pair feature importances with their corresponding feature names\n",
    "feature_names = X_subset.columns\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top ten features\n",
    "top_n = 10\n",
    "top_features = feature_importance_df.head(top_n)['Feature'].tolist()\n",
    "\n",
    "# Create new datasets with only the top features\n",
    "X_train_top = X_train[top_features]\n",
    "X_valid_top = X_subset_test[top_features]\n",
    "\n",
    "# Train a new Random Forest model using only the top features\n",
    "rf_classifier_top = RandomForestClassifier(n_estimators=450, random_state=42)\n",
    "rf_classifier_top.fit(X_train_top, y_train)\n",
    "\n",
    "# Make predictions on the validation set using the new model\n",
    "y_pred_top = rf_classifier_top.predict(X_valid_top)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_subset_test, y_pred_top)\n",
    "recall = recall_score(y_subset_test, y_pred_top)\n",
    "precision = precision_score(y_subset_test, y_pred_top)\n",
    "f1 = f1_score(y_subset_test, y_pred_top)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'F1-Score'],\n",
    "    'Value': [accuracy, recall, precision, f1]\n",
    "})\n",
    "\n",
    "# Print the metrics DataFrame\n",
    "print(\"--------RFC Model with top ten features----------\")\n",
    "print(\"\\n\")\n",
    "print(metrics_df)\n",
    "print(\"\\n\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5a014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44d5bb37",
   "metadata": {},
   "source": [
    "## 6.2 GBM Model with Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb3a5c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- GBM Model with top ten features----------\n",
      "\n",
      "\n",
      "      Metric     Value\n",
      "0   Accuracy  0.900481\n",
      "1     Recall  0.708333\n",
      "2  Precision  0.602837\n",
      "3   F1-Score  0.651341\n",
      "\n",
      "\n",
      "['duration', 'nr.employed', 'euribor3m', 'cons.conf.idx', 'campaign', 'day_of_week', 'month', 'contact', 'poutcome', 'emp.var.rate']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X and y as your feature matrix and target variable\n",
    "\n",
    "# # Split your data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Gradient Boosting Machine (GBM) model\n",
    "gbm_classifier = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "gbm_classifier.fit(X_subset, y_subset)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = gbm_classifier.feature_importances_\n",
    "\n",
    "# Pair feature importances with their corresponding feature names\n",
    "feature_names = X_subset.columns\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top ten features\n",
    "top_n = 10\n",
    "top_features = feature_importance_df.head(top_n)['Feature'].tolist()\n",
    "\n",
    "# Create new datasets with only the top features\n",
    "X_train_top = X_subset[top_features]\n",
    "X_valid_top = X_subset_test[top_features]\n",
    "\n",
    "# Train a new GBM model using only the top features\n",
    "gbm_classifier_top = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "gbm_classifier_top.fit(X_train_top, y_subset)\n",
    "\n",
    "# Make predictions on the validation set using the new model\n",
    "y_pred_top = gbm_classifier_top.predict(X_valid_top)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_subset_test, y_pred_top)\n",
    "recall = recall_score(y_subset_test, y_pred_top)\n",
    "precision = precision_score(y_subset_test, y_pred_top)\n",
    "f1 = f1_score(y_subset_test, y_pred_top)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'F1-Score'],\n",
    "    'Value': [accuracy, recall, precision, f1]\n",
    "})\n",
    "\n",
    "# Print the metrics DataFrame\n",
    "print(\"-------- GBM Model with top ten features----------\")\n",
    "print(\"\\n\")\n",
    "print(metrics_df)\n",
    "print(\"\\n\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6b385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcdd476a",
   "metadata": {},
   "source": [
    "# THE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3604da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
